{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtered 1: solely ranking based on long-tail score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cured sample size: 187883\n",
      "corrupted_samples_total: 0\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "label size: 300932\n",
      "Revised Counter(labels): Counter({3: 169656, 4: 52096, 1: 35698, 2: 25501, 0: 17884, 5: 97})\n",
      "label_wise_filter_out_samples: 0\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Size of the filtered dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 41.61ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19175072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "dataset_size=10000\n",
    "\n",
    "## label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "\n",
    "'''Part 1 (label-wise): label curation'''\n",
    "### choose the data index that needed to be remove\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "##  samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']: ##(idx, label, confidence)\n",
    "    if sample[2] >= 0: #confidence prob;0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "\n",
    "#filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"corrupted_samples_total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "\n",
    "# change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "print(f\"label size: {len(labels)}\")\n",
    "\n",
    "## select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "\n",
    "###filter out the low-quality samples\n",
    "\n",
    "low_quality_label_idx = []\n",
    "# for idx, label in enumerate(labels):\n",
    "#     ######################## select labels  ########################\n",
    "#     if label<4: \n",
    "#         low_quality_label_idx.append(idx)\n",
    "#     # elif label == 3 and random.random() >= 0.5:\n",
    "#     #     low_quality_label_idx.append(idx)\n",
    "\n",
    "\n",
    "label_wise_filter_out_samples = set(low_quality_label_idx + corrupted_samples_total)\n",
    "\n",
    "\n",
    "print(f\"label_wise_filter_out_samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "'''Part-2 (feature-wise): handle the rare example'''\n",
    "\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example'])//2]\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in label_wise_filter_out_samples] \n",
    "rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples] \n",
    "\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in set(label_wise_filter_out_samples)] \n",
    "\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "long_tail_scores = np.array(rare_samples_filtered)[:,1]\n",
    "\n",
    "\n",
    "# 根据第二个值降序排序\n",
    "sorted_samples = sorted(rare_samples_filtered, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 只保留排序后的 x[0]\n",
    "remaining_samples_indices = [x[0] for x in sorted_samples][:dataset_size]\n",
    "\n",
    "\n",
    "remaining_samples_idx = np.array(rare_samples_filtered, dtype=int)[remaining_samples_indices, 0]\n",
    "remaining_samples_idx_2 = remaining_samples_idx\n",
    "# long_tail_scores_filtered = long_tail_scores[remaining_samples_idx]\n",
    "long_tail_scores_filtered = np.array(rare_samples_filtered)[remaining_samples_indices, 1]\n",
    "\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "'''filter out the corrupted samples and reconstruct the dataset'''\n",
    "\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "\n",
    "\n",
    "# filtered_dialogs = data['train'].select(remaining_samples_idx).shuffle(seed=42)\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx)\n",
    "\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_1_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered 2: long-tail score * label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cured sample size: 187883\n",
      "corrupted_samples_total: 0\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "label size: 300932\n",
      "Revised Counter(labels): Counter({3: 169656, 4: 52096, 1: 35698, 2: 25501, 0: 17884, 5: 97})\n",
      "label_wise_filter_out_samples: 0\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Size of the filtered dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25566662"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "dataset_size=10000\n",
    "\n",
    "## label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "\n",
    "'''Part 1 (label-wise): label curation'''\n",
    "### choose the data index that needed to be remove\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "##  samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']: ##(idx, label, confidence)\n",
    "    if sample[2] >= 0: #confidence prob;0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "\n",
    "#filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"corrupted_samples_total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "\n",
    "# change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "print(f\"label size: {len(labels)}\")\n",
    "\n",
    "## select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "\n",
    "###filter out the low-quality samples\n",
    "\n",
    "low_quality_label_idx = []\n",
    "# for idx, label in enumerate(labels):\n",
    "#     ######################## select labels  ########################\n",
    "#     if label<4: \n",
    "#         low_quality_label_idx.append(idx)\n",
    "#     # elif label == 3 and random.random() >= 0.5:\n",
    "#     #     low_quality_label_idx.append(idx)\n",
    "\n",
    "\n",
    "label_wise_filter_out_samples = set(low_quality_label_idx + corrupted_samples_total)\n",
    "\n",
    "\n",
    "print(f\"label_wise_filter_out_samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "'''Part-2 (feature-wise): handle the rare example'''\n",
    "\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example'])//2]\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in label_wise_filter_out_samples] \n",
    "\n",
    "\n",
    "rare_samples_filtered = [[sample[0], sample[1] * labels[sample[0]]] for sample in rare_samples] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "long_tail_scores = np.array(rare_samples_filtered)[:,1]\n",
    "\n",
    "\n",
    "# 根据第二个值降序排序\n",
    "sorted_samples = sorted(rare_samples_filtered, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 只保留排序后的 x[0]\n",
    "remaining_samples_indices = [x[0] for x in sorted_samples][:dataset_size]\n",
    "\n",
    "\n",
    "remaining_samples_idx = np.array(rare_samples_filtered, dtype=int)[remaining_samples_indices, 0]\n",
    "remaining_samples_idx_2 = remaining_samples_idx\n",
    "# long_tail_scores_filtered = long_tail_scores[remaining_samples_idx]\n",
    "long_tail_scores_filtered = np.array(rare_samples_filtered)[remaining_samples_indices, 1]\n",
    "\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "'''filter out the corrupted samples and reconstruct the dataset'''\n",
    "\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "\n",
    "\n",
    "# filtered_dialogs = data['train'].select(remaining_samples_idx).shuffle(seed=42)\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx)\n",
    "\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_2_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered 3: reduce the bin scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cured sample size: 187883\n",
      "corrupted_samples_total: 0\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "label size: 300932\n",
      "Revised Counter(labels): Counter({3: 169656, 4: 52096, 1: 35698, 2: 25501, 0: 17884, 5: 97})\n",
      "label_wise_filter_out_samples: 0\n",
      "Size of the remaining samples with high quality: 300932\n",
      "threshold - len(high_quality_indices_in_bin: 2499;;; len_low: 335\n",
      "### the last remain sample count: 335\n",
      "threshold - len(high_quality_indices_in_bin: 2495;;; len_low: 1849\n",
      "### the last remain sample count: 1849\n",
      "threshold - len(high_quality_indices_in_bin: 2481;;; len_low: 12247\n",
      "threshold - len(high_quality_indices_in_bin: 2450;;; len_low: 28625\n",
      "threshold - len(high_quality_indices_in_bin: 2479;;; len_low: 8704\n",
      "threshold - len(high_quality_indices_in_bin: 2499;;; len_low: 333\n",
      "### the last remain sample count: 333\n",
      "Size of the filtered dataset: 10065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 39.34ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26694206"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "## label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "\n",
    "'''Part 1 (label-wise): label curation'''\n",
    "### choose the data index that needed to be remove\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "##  samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']: ##(idx, label, confidence)\n",
    "    if sample[2] >= 0: #confidence prob;0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "\n",
    "#filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"corrupted_samples_total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "\n",
    "# change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "print(f\"label size: {len(labels)}\")\n",
    "\n",
    "## select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "\n",
    "###filter out the low-quality samples\n",
    "\n",
    "low_quality_label_idx = []\n",
    "# for idx, label in enumerate(labels):\n",
    "#     ######################## select labels  ########################\n",
    "#     if label<4: \n",
    "#         low_quality_label_idx.append(idx)\n",
    "#     # elif label == 3 and random.random() >= 0.5:\n",
    "#     #     low_quality_label_idx.append(idx)\n",
    "\n",
    "\n",
    "label_wise_filter_out_samples = set(low_quality_label_idx + corrupted_samples_total)\n",
    "\n",
    "\n",
    "print(f\"label_wise_filter_out_samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "'''Part-2 (feature-wise): handle the rare example'''\n",
    "\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example'])//2]\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in label_wise_filter_out_samples] \n",
    "rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples] \n",
    "\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in set(label_wise_filter_out_samples)] \n",
    "\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "long_tail_scores = np.array(rare_samples_filtered)[:,1]\n",
    "\n",
    "bins = np.arange(0, max(long_tail_scores)+0.03, 0.03) # 定义区间边界\n",
    "\n",
    "# 计算每个区间的计数\n",
    "counts, _ = np.histogram(long_tail_scores, bins)\n",
    "\n",
    "\n",
    "threshold = 2500 ## the random threshold for feature-wise\n",
    "\n",
    "remaining_samples_indices = []\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# '''Only select the high-rated (5) samples '''\n",
    "# for i in range(len(bins) - 1):\n",
    "#     indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "#     # if counts[i] > threshold:\n",
    "#     #     # indices_in_bin = [idx for idx in indices_in_bin if idx not in label_wise_filter_out_samples] ## only remove the wrong-annotated samples if the sample size is too much, otherwise remain them.\n",
    "\n",
    "#     #     high_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == 5]\n",
    "\n",
    "#     #     # if len(high_quality_indices_in_bin) >= threshold:\n",
    "\n",
    "#     #     #     high_quality_indices_in_bin = random.sample(list(high_quality_indices_in_bin), threshold)\n",
    "\n",
    "#     #     remaining_samples_indices.extend(high_quality_indices_in_bin)\n",
    "#     #     #     low_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] >= 3]\n",
    "#     #     #     print(f\"threshold - len(high_quality_indices_in_bin: {threshold - len(high_quality_indices_in_bin)};;; len_low: {len(low_quality_indices_in_bin)}\")\n",
    "#     #     #     low_quality_indices_in_bin = random.sample(list(low_quality_indices_in_bin), threshold - len(high_quality_indices_in_bin))\n",
    "#     #     #     remaining_samples_indices.extend(high_quality_indices_in_bin + low_quality_indices_in_bin)\n",
    "\n",
    "#     # else:\n",
    "#         # \n",
    "#     high_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == 5]\n",
    "\n",
    "#     remaining_samples_indices.extend(high_quality_indices_in_bin)\n",
    "\n",
    "########################################################################################################################\n",
    "filter_idx=0\n",
    "\n",
    "\n",
    "for i in range(len(bins) - 1):\n",
    "    \n",
    "    if i <filter_idx: ## remove some too similar samples\n",
    "\n",
    "        indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "\n",
    "        high_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] >= 4]\n",
    "\n",
    "        similar_sample_idxs = random.sample(list(high_quality_indices_in_bin), int(5 * 100 * bins[i]))\n",
    "        print(f\"#### original high-rated sample count {len(high_quality_indices_in_bin)} ;  #### selected  similar samples: {len(similar_sample_idxs)}\")\n",
    "        remaining_samples_indices.extend(similar_sample_idxs)\n",
    "    else:\n",
    "        indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "        if counts[i] > threshold:\n",
    "            # indices_in_bin = [idx for idx in indices_in_bin if idx not in label_wise_filter_out_samples] ## only remove the wrong-annotated samples if the sample size is too much, otherwise remain them.\n",
    "\n",
    "            high_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == 5]\n",
    "\n",
    "            if len(high_quality_indices_in_bin) >= threshold:## if the number of high-rated samples is  enough\n",
    "\n",
    "                high_quality_indices_in_bin = random.sample(list(high_quality_indices_in_bin), threshold)\n",
    "                remaining_samples_indices.extend(high_quality_indices_in_bin)\n",
    "\n",
    "            else: ## if the number of high-rated samples is not enough\n",
    "                low_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == 4]\n",
    "                print(f\"threshold - len(high_quality_indices_in_bin: {threshold - len(high_quality_indices_in_bin)};;; len_low: {len(low_quality_indices_in_bin)}\")\n",
    "                if len(low_quality_indices_in_bin) > threshold - len(high_quality_indices_in_bin):\n",
    "                    low_quality_indices_in_bin = random.sample(list(low_quality_indices_in_bin), threshold - len(high_quality_indices_in_bin))\n",
    "\n",
    "                else:\n",
    "                    low_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] >=4 and labels[rare_samples_filtered[idx][0]] !=5]\n",
    "\n",
    "                    print(f\"### the last remain sample count: {len(low_quality_indices_in_bin)}\")\n",
    "\n",
    "                remaining_samples_indices.extend(high_quality_indices_in_bin + low_quality_indices_in_bin)\n",
    "\n",
    "        else:\n",
    "            remaining_samples_indices.extend(indices_in_bin)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "# for i in range(len(bins) - 1):\n",
    "#     indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "#     if counts[i] > threshold:\n",
    "#         indices_in_bin = [idx for idx in indices_in_bin if idx not in label_wise_filter_out_samples] ## only remove the wrong-annotated samples if the sample size is too much, otherwise remain them.\n",
    "#         high_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] >= 4]\n",
    "#         low_quality_indices_in_bin = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] <= 3]\n",
    "#         if len(high_quality_indices_in_bin) > threshold//2:\n",
    "#             high_quality_indices_in_bin = random.sample(list(high_quality_indices_in_bin), threshold//2) \n",
    "#         low_quality_indices_in_bin = random.sample(list(low_quality_indices_in_bin), threshold //2)\n",
    "\n",
    "#         remaining_samples_indices.extend(high_quality_indices_in_bin + low_quality_indices_in_bin)\n",
    "\n",
    "#     else:\n",
    "#         remaining_samples_indices.extend(indices_in_bin)\n",
    "\n",
    "########################################################################################################################\n",
    "# for i in range(len(bins) - 1):\n",
    "#     indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "#     if counts[i] > threshold:\n",
    "#         new_indices_in_bin = random.sample(list(indices_in_bin), threshold)\n",
    "#         remaining_samples_indices.extend(new_indices_in_bin)\n",
    "\n",
    "#     else:\n",
    "#         remaining_samples_indices.extend(indices_in_bin)\n",
    "########################################################################################################################\n",
    "\n",
    "# label_to_indices = {i: [] for i in range(1, 6)}\n",
    "\n",
    "# for i in range(len(bins) - 1):\n",
    "#     indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i + 1]))[0]\n",
    "#     if counts[i] > threshold:\n",
    "#         # 将样本索引按标签分类存储到字典中\n",
    "#         for idx in indices_in_bin:\n",
    "#             label = labels[rare_samples_filtered[idx][0]]\n",
    "#             if 1 <= label <= 5:\n",
    "#                 label_to_indices[label].append(idx)\n",
    "        \n",
    "#         # 从每个标签中采样\n",
    "#         for label in range(3, 6):\n",
    "#             indices = label_to_indices[label]\n",
    "#             if len(indices) >= threshold // 3:\n",
    "#                 sampled_indices = random.sample(indices, threshold // 3)\n",
    "#             else:\n",
    "#                 sampled_indices = indices  # 如果样本数不够 threshold // 5，取全部样本\n",
    "#             remaining_samples_indices.extend(sampled_indices)\n",
    "            \n",
    "########################################################################################################################\n",
    "\n",
    "remaining_samples_idx = np.array(rare_samples_filtered, dtype=int)[remaining_samples_indices, 0]\n",
    "remaining_samples_idx_2 = remaining_samples_idx\n",
    "# long_tail_scores_filtered = long_tail_scores[remaining_samples_idx]\n",
    "long_tail_scores_filtered = np.array(rare_samples_filtered)[remaining_samples_indices, 1]\n",
    "\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "'''filter out the corrupted samples and reconstruct the dataset'''\n",
    "\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "\n",
    "\n",
    "# filtered_dialogs = data['train'].select(remaining_samples_idx).shuffle(seed=42)\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx)\n",
    "\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_3_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtered 4: long-tail score * label form a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "cured sample size: 0\n",
      "corrupted_samples_total: 187883\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "label size: 300932\n",
      "Revised Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "label_wise_filter_out_samples: 187883\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Bin 0 - total size: 27879 ---high-rated samples 4 --- Total samples selected: 182\n",
      "Bin 1 - total size: 1032 ---high-rated samples 6 --- Total samples selected: 19\n",
      "Bin 2 - total size: 1753 ---high-rated samples 1 --- Total samples selected: 23\n",
      "Bin 3 - total size: 2267 ---high-rated samples 3 --- Total samples selected: 31\n",
      "Bin 4 - total size: 3149 ---high-rated samples 4 --- Total samples selected: 43\n",
      "Bin 5 - total size: 4350 ---high-rated samples 7 --- Total samples selected: 165\n",
      "Bin 6 - total size: 5886 ---high-rated samples 5 --- Total samples selected: 219\n",
      "Bin 7 - total size: 7718 ---high-rated samples 7 --- Total samples selected: 286\n",
      "Bin 8 - total size: 8509 ---high-rated samples 10 --- Total samples selected: 318\n",
      "Bin 9 - total size: 8225 ---high-rated samples 6 --- Total samples selected: 303\n",
      "Bin 10 - total size: 7350 ---high-rated samples 10 --- Total samples selected: 276\n",
      "Bin 11 - total size: 6334 ---high-rated samples 7 --- Total samples selected: 236\n",
      "Bin 12 - total size: 5526 ---high-rated samples 6 --- Total samples selected: 207\n",
      "Bin 13 - total size: 5040 ---high-rated samples 9 --- Total samples selected: 191\n",
      "Bin 14 - total size: 5329 ---high-rated samples 7 --- Total samples selected: 200\n",
      "Bin 15 - total size: 5510 ---high-rated samples 14 --- Total samples selected: 213\n",
      "Bin 16 - total size: 5723 ---high-rated samples 6 --- Total samples selected: 213\n",
      "Bin 17 - total size: 5879 ---high-rated samples 10 --- Total samples selected: 223\n",
      "Bin 18 - total size: 6342 ---high-rated samples 12 --- Total samples selected: 242\n",
      "Bin 19 - total size: 6534 ---high-rated samples 12 --- Total samples selected: 248\n",
      "Bin 20 - total size: 6853 ---high-rated samples 19 --- Total samples selected: 267\n",
      "Bin 21 - total size: 7195 ---high-rated samples 18 --- Total samples selected: 278\n",
      "Bin 22 - total size: 6977 ---high-rated samples 20 --- Total samples selected: 273\n",
      "Bin 23 - total size: 7170 ---high-rated samples 19 --- Total samples selected: 278\n",
      "Bin 24 - total size: 7537 ---high-rated samples 19 --- Total samples selected: 291\n",
      "Bin 25 - total size: 7304 ---high-rated samples 21 --- Total samples selected: 286\n",
      "Bin 26 - total size: 7104 ---high-rated samples 29 --- Total samples selected: 286\n",
      "Bin 27 - total size: 7657 ---high-rated samples 34 --- Total samples selected: 311\n",
      "Bin 28 - total size: 7671 ---high-rated samples 34 --- Total samples selected: 312\n",
      "Bin 29 - total size: 7312 ---high-rated samples 36 --- Total samples selected: 301\n",
      "Bin 30 - total size: 7529 ---high-rated samples 44 --- Total samples selected: 316\n",
      "Bin 31 - total size: 7456 ---high-rated samples 49 --- Total samples selected: 319\n",
      "Bin 32 - total size: 7006 ---high-rated samples 41 --- Total samples selected: 295\n",
      "Bin 33 - total size: 6866 ---high-rated samples 61 --- Total samples selected: 309\n",
      "Bin 34 - total size: 6405 ---high-rated samples 64 --- Total samples selected: 296\n",
      "Bin 35 - total size: 5928 ---high-rated samples 67 --- Total samples selected: 282\n",
      "Bin 36 - total size: 5807 ---high-rated samples 55 --- Total samples selected: 265\n",
      "Bin 37 - total size: 5191 ---high-rated samples 75 --- Total samples selected: 263\n",
      "Bin 38 - total size: 4923 ---high-rated samples 89 --- Total samples selected: 268\n",
      "Bin 39 - total size: 4557 ---high-rated samples 94 --- Total samples selected: 259\n",
      "Bin 40 - total size: 4221 ---high-rated samples 91 --- Total samples selected: 244\n",
      "Bin 41 - total size: 3770 ---high-rated samples 87 --- Total samples selected: 223\n",
      "Bin 42 - total size: 3663 ---high-rated samples 126 --- Total samples selected: 259\n",
      "Bin 43 - total size: 3177 ---high-rated samples 120 --- Total samples selected: 236\n",
      "Bin 44 - total size: 2993 ---high-rated samples 145 --- Total samples selected: 253\n",
      "Bin 45 - total size: 2566 ---high-rated samples 137 --- Total samples selected: 230\n",
      "Bin 46 - total size: 2283 ---high-rated samples 136 --- Total samples selected: 219\n",
      "Bin 47 - total size: 1964 ---high-rated samples 123 --- Total samples selected: 194\n",
      "Bin 48 - total size: 1843 ---high-rated samples 125 --- Total samples selected: 192\n",
      "Bin 49 - total size: 1564 ---high-rated samples 153 --- Total samples selected: 210\n",
      "Bin 50 - total size: 1342 ---high-rated samples 136 --- Total samples selected: 185\n",
      "Bin 51 - total size: 1202 ---high-rated samples 173 --- Total samples selected: 217\n",
      "Bin 52 - total size: 989 ---high-rated samples 159 --- Total samples selected: 195\n",
      "Bin 53 - total size: 838 ---high-rated samples 135 --- Total samples selected: 166\n",
      "Bin 54 - total size: 666 ---high-rated samples 118 --- Total samples selected: 142\n",
      "Bin 55 - total size: 583 ---high-rated samples 123 --- Total samples selected: 144\n",
      "Bin 56 - total size: 486 ---high-rated samples 120 --- Total samples selected: 138\n",
      "Bin 57 - total size: 384 ---high-rated samples 126 --- Total samples selected: 141\n",
      "Bin 58 - total size: 308 ---high-rated samples 91 --- Total samples selected: 102\n",
      "Bin 59 - total size: 232 ---high-rated samples 80 --- Total samples selected: 89\n",
      "Bin 60 - total size: 225 ---high-rated samples 88 --- Total samples selected: 96\n",
      "Bin 61 - total size: 165 ---high-rated samples 76 --- Total samples selected: 82\n",
      "Bin 62 - total size: 147 ---high-rated samples 76 --- Total samples selected: 82\n",
      "Bin 63 - total size: 106 ---high-rated samples 50 --- Total samples selected: 55\n",
      "Bin 64 - total size: 89 ---high-rated samples 44 --- Total samples selected: 48\n",
      "Bin 65 - total size: 78 ---high-rated samples 43 --- Total samples selected: 47\n",
      "Bin 66 - total size: 63 ---high-rated samples 34 --- Total samples selected: 37\n",
      "Bin 67 - total size: 44 ---high-rated samples 28 --- Total samples selected: 30\n",
      "Bin 68 - total size: 31 ---high-rated samples 23 --- Total samples selected: 25\n",
      "Bin 69 - total size: 29 ---high-rated samples 21 --- Total samples selected: 23\n",
      "Bin 70 - total size: 25 ---high-rated samples 19 --- Total samples selected: 21\n",
      "Bin 71 - total size: 13 ---high-rated samples 10 --- Total samples selected: 10\n",
      "Bin 72 - total size: 19 ---high-rated samples 19 --- Total samples selected: 21\n",
      "Bin 73 - total size: 5 ---high-rated samples 3 --- Total samples selected: 3\n",
      "Bin 74 - total size: 9 ---high-rated samples 8 --- Total samples selected: 8\n",
      "Bin 75 - total size: 7 ---high-rated samples 7 --- Total samples selected: 7\n",
      "Bin 76 - total size: 5 ---high-rated samples 5 --- Total samples selected: 5\n",
      "Bin 77 - total size: 5 ---high-rated samples 5 --- Total samples selected: 5\n",
      "Bin 78 - total size: 3 ---high-rated samples 3 --- Total samples selected: 3\n",
      "Bin 79 - total size: 1 ---high-rated samples 1 --- Total samples selected: 1\n",
      "Bin 80 - total size: 1 ---high-rated samples 1 --- Total samples selected: 1\n",
      "Bin 82 - total size: 2 ---high-rated samples 2 --- Total samples selected: 2\n",
      "Bin 84 - total size: 1 ---high-rated samples 1 --- Total samples selected: 1\n",
      "Bin 86 - total size: 1 ---high-rated samples 1 --- Total samples selected: 1\n",
      "Bin 89 - total size: 1 ---high-rated samples 1 --- Total samples selected: 1\n",
      "Size of the filtered dataset: 9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 32.35ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20425224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "dataset_size =10000\n",
    "\n",
    "## label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "\n",
    "'''Part 1 (label-wise): label curation'''\n",
    "### choose the data index that needed to be remove\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "##  samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']: ##(idx, label, confidence)\n",
    "    if sample[2] >= 1: #confidence prob;0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "\n",
    "#filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"corrupted_samples_total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "\n",
    "# change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "print(f\"label size: {len(labels)}\")\n",
    "\n",
    "## select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "\n",
    "\n",
    "###filter out the low-quality samples\n",
    "\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "\n",
    "\n",
    "print(f\"label_wise_filter_out_samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "'''Part-2 (feature-wise): handle the rare example'''\n",
    "\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example'])//2]\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in label_wise_filter_out_samples] \n",
    "rare_samples_filtered = [[sample[0], sample[1] * labels[sample[0]]] for sample in rare_samples] \n",
    "\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples if sample[0] not in set(label_wise_filter_out_samples)] \n",
    "\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "long_tail_scores = np.array(rare_samples_filtered)[:,1]\n",
    "\n",
    "bins = np.arange(0, max(long_tail_scores)+0.01, 0.01) # 定义区间边界\n",
    "\n",
    "# 计算每个区间的计数\n",
    "counts, _ = np.histogram(long_tail_scores, bins)\n",
    "\n",
    "##################################################################################################################\n",
    "#### data proportion #####\n",
    "\n",
    "count_bins = []\n",
    "# 定义前五个 bins 的权重和后面 bins 的权重\n",
    "front_bin_weight = 1\n",
    "mid_bin_weight= 3\n",
    "back_bin_weight = 1\n",
    "remain_data_size = dataset_size - Counter(labels)[5]\n",
    "\n",
    "# 计算每个 bin 的比例\n",
    "for i in range(0, len(bins) - 1):\n",
    "    indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "    count_bins.append(round(len(indices_in_bin) / len(long_tail_scores), 4))\n",
    "\n",
    "# 调整前五个 bins 和后面 bins 的比例\n",
    "adjusted_count_bins = []\n",
    "for i, count_bin in enumerate(count_bins): # [0,18]\n",
    "    if i < 5:\n",
    "        adjusted_count_bin = count_bin * front_bin_weight\n",
    "    elif i > len(bins) -5:\n",
    "        adjusted_count_bin = count_bin * back_bin_weight\n",
    "    else:\n",
    "        adjusted_count_bin = count_bin * mid_bin_weight\n",
    "\n",
    "    adjusted_count_bins.append(adjusted_count_bin)\n",
    "\n",
    "\n",
    "# 正规化，使得调整后的比例和为1\n",
    "total_adjusted = sum(adjusted_count_bins)\n",
    "adjusted_count_bins = [bin_count / total_adjusted for bin_count in adjusted_count_bins]\n",
    "\n",
    "import math\n",
    "# 根据调整后的比例计算 bins_threshold\n",
    "bins_threshold = [math.ceil(count_bin * dataset_size) for count_bin in adjusted_count_bins]\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "#### selection strategy\n",
    "remaining_samples_indices = []\n",
    "\n",
    "for i in range(0, len(bins) - 1):\n",
    "    indices_in_bin = np.where((long_tail_scores >= bins[i]) & (long_tail_scores < bins[i+1]))[0]\n",
    "    # 计算当前 bin 的样本阈值\n",
    "    high_quality_indices_5 = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == 5]\n",
    "\n",
    "\n",
    "    current_threshold = bins_threshold[i]\n",
    "    \n",
    "    # 如果当前 bin 的样本数量已经满足阈值，则不进行操作\n",
    "    if len(indices_in_bin) <= current_threshold:\n",
    "        remaining_samples_indices.extend(indices_in_bin)\n",
    "        continue\n",
    "    \n",
    "    # 从高到低优先选择标签值高的样本\n",
    "    selected_indices = []\n",
    "    for label in range(5, 2, -1):  # 从标签4到标签1\n",
    "        high_quality_indices = [idx for idx in indices_in_bin if labels[rare_samples_filtered[idx][0]] == label]\n",
    "        \n",
    "        if len(selected_indices) + len(high_quality_indices) <= current_threshold:\n",
    "            selected_indices.extend(high_quality_indices)\n",
    "        else:\n",
    "            needed = current_threshold - len(selected_indices)\n",
    "            selected_indices.extend(random.sample(high_quality_indices, needed))\n",
    "            break\n",
    "    \n",
    "    remaining_samples_indices.extend(selected_indices)\n",
    "    print(f\"Bin {i} - total size: {len(indices_in_bin)} ---high-rated samples {len(high_quality_indices_5)} --- Total samples selected: {len(high_quality_indices_5 + selected_indices)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "remaining_samples_idx = np.array(rare_samples_filtered, dtype=int)[remaining_samples_indices, 0]\n",
    "remaining_samples_idx_2 = remaining_samples_idx\n",
    "# long_tail_scores_filtered = long_tail_scores[remaining_samples_idx]\n",
    "long_tail_scores_filtered = np.array(rare_samples_filtered)[remaining_samples_indices, 1]\n",
    "\n",
    "# 打印剩余的样本及其原始索引\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "'''filter out the corrupted samples and reconstruct the dataset'''\n",
    "\n",
    "###the parquet data path\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "\n",
    "\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx)\n",
    "\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_4_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered 5: label-filtered based: all 5 samples  + 4-rated samples select using sorted long-tail score (reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "Cured sample size: 0\n",
      "Corrupted samples total: 187883\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "Label-wise filter out samples: 187883\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Size of the filtered dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 300932 examples [00:05, 53968.34 examples/s]\n",
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 34.03ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26002307"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(3)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "dataset_size = 10000\n",
    "\n",
    "# label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "# Choose the data index that needs to be removed\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "# Samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= 1:  # confidence prob; 0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "\n",
    "# Select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "# Part 2 (feature-wise): handle the rare example\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples]\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "label_5_indices = [idx for idx in range(len(labels)) if labels[idx] == 5]\n",
    "label_4_indices = [idx for idx in range(len(labels)) if labels[idx] == 4]\n",
    "\n",
    "sample_rated_4 = [[sample[0], sample[1]] for sample in rare_samples if sample[0] in set(label_4_indices)]\n",
    "\n",
    "remain_data_size = dataset_size - len(label_5_indices)\n",
    "\n",
    "sorted_samples_rated_4 = sorted(sample_rated_4, key=lambda x: x[1], reverse=True)[:remain_data_size]\n",
    "\n",
    "sorted_samples_rated_4_indices = np.array(sorted_samples_rated_4)[:, 0].astype(int)\n",
    "\n",
    "remaining_samples_idx = np.concatenate([sorted_samples_rated_4_indices, label_5_indices])\n",
    "\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "# Filter out the corrupted samples and reconstruct the dataset\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx.tolist())\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_5_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2971, 0.1253]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples_rated_4[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_samples_rated_4 = sorted(sample_rated_4, key=lambda x: x[1], reverse=True)[:remain_data_size]\n",
    "\n",
    "sorted_samples_rated_all = sorted(rare_samples_filtered, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[238395, 0.1391]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples_rated_all[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered 6: label-filtered based: all 5 samples  + 4-rated samples select using sorted long-tail score (reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "Cured sample size: 0\n",
      "Corrupted samples total: 187883\n",
      "Original Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n",
      "Label-wise filter out samples: 187883\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Size of the filtered dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 35.59ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23231536"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "dataset_size = 10000\n",
    "\n",
    "# label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "# Choose the data index that needs to be removed\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "# Samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= 1:  # confidence prob; 0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "\n",
    "# Select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "# Part 2 (feature-wise): handle the rare example\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples]\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "label_5_indices = [idx for idx in range(len(labels)) if labels[idx] == 5]\n",
    "label_4_indices = [idx for idx in range(len(labels)) if labels[idx] == 4]\n",
    "\n",
    "sample_rated_4 = [[sample[0], sample[1]] for sample in rare_samples if sample[0] in set(label_4_indices)]\n",
    "\n",
    "remain_data_size = dataset_size - len(label_5_indices)\n",
    "\n",
    "sorted_samples_rated_4 = sorted(sample_rated_4, key=lambda x: x[1], reverse=False)[:remain_data_size]\n",
    "\n",
    "sorted_samples_rated_4_indices = np.array(sorted_samples_rated_4)[:, 0].astype(int)\n",
    "\n",
    "remaining_samples_idx = np.concatenate([sorted_samples_rated_4_indices, label_5_indices])\n",
    "\n",
    "print(\"Size of the filtered dataset:\", len(remaining_samples_idx))\n",
    "\n",
    "# Filter out the corrupted samples and reconstruct the dataset\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "filtered_dialogs = data['train'].select(remaining_samples_idx.tolist())\n",
    "\n",
    "filtered_labels = np.array(labels)[remaining_samples_idx].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered_6_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 7: label-filtered + different random seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 34.53ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25784325"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "random.seed(1)\n",
    "\n",
    "dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size =10000\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/{dataset_name}_data.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "label_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/output_labels_revised.pt\"\n",
    "labels = torch.load(label_path)\n",
    "\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Counter(labels): {label_counts}\")\n",
    "\n",
    "\n",
    "# 获取所有标签为 5 的索引\n",
    "index_5 = [i for i, label in enumerate(labels) if label == 5]\n",
    "\n",
    "# 如果已经有10000个索引则直接返回\n",
    "if len(index_5) >= dataset_size:\n",
    "    selected_indices = index_5[:dataset_size]\n",
    "else:\n",
    "\n",
    "    # 获取所有标签为 4 的索引\n",
    "    index_4 = [i for i, label in enumerate(labels) if label == 4]\n",
    "\n",
    "    random_indices_4 = random.sample(index_4,  dataset_size - len(index_5))\n",
    "\n",
    "    label_filtered_indices = index_5 + random_indices_4\n",
    "\n",
    "\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"filtered_7_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 8: label-filtered + different random seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 36.37ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25557575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "random.seed(2)\n",
    "\n",
    "dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size =10000\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/{dataset_name}_data.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "label_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/output_labels_revised.pt\"\n",
    "labels = torch.load(label_path)\n",
    "\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Counter(labels): {label_counts}\")\n",
    "\n",
    "\n",
    "# 获取所有标签为 5 的索引\n",
    "index_5 = [i for i, label in enumerate(labels) if label == 5]\n",
    "\n",
    "# 如果已经有10000个索引则直接返回\n",
    "if len(index_5) >= dataset_size:\n",
    "    selected_indices = index_5[:dataset_size]\n",
    "else:\n",
    "\n",
    "    # 获取所有标签为 4 的索引\n",
    "    index_4 = [i for i, label in enumerate(labels) if label == 4]\n",
    "\n",
    "    random_indices_4 = random.sample(index_4,  dataset_size - len(index_5))\n",
    "\n",
    "    label_filtered_indices = index_5 + random_indices_4\n",
    "\n",
    "\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"filtered_8_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 9: label-filtered + different random seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(labels): Counter({3: 116114, 4: 57669, 2: 48254, 1: 47402, 0: 27386, 5: 4107})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 37.79ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25964083"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "random.seed(3)\n",
    "\n",
    "dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size =10000\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/{dataset_name}_data.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "label_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/output_labels_revised.pt\"\n",
    "labels = torch.load(label_path)\n",
    "\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Counter(labels): {label_counts}\")\n",
    "\n",
    "\n",
    "# 获取所有标签为 5 的索引\n",
    "index_5 = [i for i, label in enumerate(labels) if label == 5]\n",
    "\n",
    "# 如果已经有10000个索引则直接返回\n",
    "if len(index_5) >= dataset_size:\n",
    "    selected_indices = index_5[:dataset_size]\n",
    "else:\n",
    "\n",
    "    # 获取所有标签为 4 的索引\n",
    "    index_4 = [i for i, label in enumerate(labels) if label == 4]\n",
    "\n",
    "    random_indices_4 = random.sample(index_4,  dataset_size - len(index_5))\n",
    "\n",
    "    label_filtered_indices = index_5 + random_indices_4\n",
    "\n",
    "\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"filtered_9_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline + different random seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 301/301 [00:04<00:00, 61.62ba/s]\n",
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 50.65ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17362444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "#### \n",
    "dataset_size = 10000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "json_dir = '/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(random_indices)\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset.to_json(root_path + f\"full_dataset.json\")\n",
    "\n",
    "random_dataset.to_json(root_path + f\"random_1_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline + different random seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 301/301 [00:04<00:00, 68.60ba/s]\n",
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 44.77ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17215744"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "\n",
    "#### \n",
    "dataset_size = 10000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "json_dir = '/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(random_indices)\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset.to_json(root_path + f\"full_dataset.json\")\n",
    "\n",
    "random_dataset.to_json(root_path + f\"random_2_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 301/301 [00:03<00:00, 75.79ba/s]\n",
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 45.02ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17538885"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(5)\n",
    "\n",
    "\n",
    "#### \n",
    "dataset_size = 10000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "json_dir = '/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(random_indices)\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset.to_json(root_path + f\"full_dataset.json\")\n",
    "\n",
    "random_dataset.to_json(root_path + f\"random_5_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(3)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "dataset_size = 5000\n",
    "\n",
    "# label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "# Choose the data index that needs to be removed\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "# Samples that can be cured\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= 1:  # confidence prob; 0.75\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "\n",
    "# Select high-quality samples based on the quality labels\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "# Part 2 (feature-wise): handle the rare example\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples]\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "\n",
    "filtered_indices = []\n",
    "\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    label_indices = [idx for idx in range(len(labels)) if labels[idx] == target_label]\n",
    "\n",
    "    if dataset_size - len(filtered_indices) > len(label_indices):\n",
    "        filtered_indices.extend(label_indices)\n",
    "    else:\n",
    "        rated_samples = [[sample[0], sample[1]] for sample in rare_samples if sample[0] in set(label_indices)]\n",
    "\n",
    "        remain_data_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "        sorted_samples = sorted(rated_samples, key=lambda x: x[1], reverse=True)[:remain_data_size]\n",
    "\n",
    "        sorted_samples_indices = np.array(sorted_samples)[:, 0].astype(int)\n",
    "\n",
    "        filtered_indices.extend(sorted_samples_indices)\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# Filter out the corrupted samples and reconstruct the dataset\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "\n",
    "# Filter out the corrupted samples and reconstruct the dataset\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "filtered_dialogs = data['train'].select(filtered_indices.tolist())\n",
    "\n",
    "filtered_labels = np.array(labels)[filtered_indices].tolist()\n",
    "\n",
    "assert len(filtered_dialogs) == len(filtered_labels)\n",
    "\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-{dataset_size//1000}k_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
