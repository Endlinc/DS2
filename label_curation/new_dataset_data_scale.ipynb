{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 301/301 [00:04<00:00, 73.91ba/s]\n",
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 46.72ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4339289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "#### \n",
    "dataset_size = 5000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "json_dir = '/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "\n",
    "\n",
    "# random_indices = random.sample(len(all_train_dataset), dataset_size)\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(random_indices)\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset.to_json(root_path + f\"full_dataset.json\")\n",
    "\n",
    "random_dataset.to_json(root_path + f\"random-{dataset_size//1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: LESS (Influence-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "task='mmlu'\n",
    "tot_dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "dataset_size =40000\n",
    "\n",
    "\n",
    "DATASET_LIST = ['flan_v2', 'oasst1', 'wizardlm', 'dolly', 'stanford_alpaca'] #all_train\n",
    "# DATASET_LIST = ['flan_v2'] #all_train\n",
    "\n",
    "\n",
    "task_list = [\"mmlu\"]\n",
    "infl_score_all_tasks= []\n",
    "for task in task_list:\n",
    "    infl_score_all = []\n",
    "    for dataset_name in DATASET_LIST:\n",
    "        root_path = f\"less-outputs/selected_data/{task}/\"\n",
    "        infl_scores = torch.load(root_path + f\"{dataset_name}_influence_score.pt\")\n",
    "        if  dataset_name == 'dolly':\n",
    "            infl_scores = infl_scores[:-40]\n",
    "        print(f\"dataset: {dataset_name} shape: {infl_scores.shape}\")\n",
    "\n",
    "        infl_score_all.append(infl_scores)\n",
    "\n",
    "    infl_score_all = torch.cat(infl_score_all, dim=0)\n",
    "    # infl_score_all_tasks.append(infl_score_all)\n",
    "\n",
    "# infl_score_all_tasks = torch.cat(infl_score_all_tasks, dim=-1)\n",
    "values, indices = torch.topk(infl_score_all, k=dataset_size)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{tot_dataset_name}/\"\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "less_dataset = data['train'].select(indices)\n",
    "\n",
    "less_dataset.to_json(root_path + f\"less-{dataset_size//1000}k_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Label-filtered algorithm \n",
    "\n",
    "- tag: label-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "data size: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 42.76ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66928993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name='all_train'\n",
    "# model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "dataset_size = 40000\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/{dataset_name}_data.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "label_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/output_labels_revised.pt\"\n",
    "labels = torch.load(label_path)\n",
    "\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Counter(labels): {label_counts}\")\n",
    "\n",
    "\n",
    "label_filtered_indices = []\n",
    "for target_label in label_counts.keys():\n",
    "    if len(label_filtered_indices) == dataset_size:\n",
    "        break   \n",
    "\n",
    "    indices = [i for i, label in enumerate(labels) if label == target_label]\n",
    "\n",
    "    if dataset_size - len(label_filtered_indices) > len(indices):\n",
    "        label_filtered_indices.extend(indices)\n",
    "    else:\n",
    "        random_indices = np.random.permutation(len(indices))[:dataset_size-len(label_filtered_indices)]\n",
    "        label_filtered_indices.extend(random_indices)\n",
    "\n",
    "print(f\"data size: {len(label_filtered_indices)}\")\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"label-filtered-{dataset_size//1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: diversity-filtered\n",
    "\n",
    "- tag: diversity-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=5): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3829/3829 [00:03<00:00, 1216.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 5: 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59969/59969 [00:48<00:00, 1223.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 4: 6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=3): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87975/87975 [01:22<00:00, 1060.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 3: 25099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=2):  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 44717/86132 [00:46<00:42, 970.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 2: 40000\n",
      "Selected data size: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:07<00:00,  5.70ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "713261521"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def cosDistance(sample_embedding, selected_embeddings, k_near=10):\n",
    "    # 确保在GPU上执行\n",
    "    sample_embedding = sample_embedding.to(device)\n",
    "    selected_embeddings = selected_embeddings.to(device)\n",
    "\n",
    "    similarity_vector = torch.matmul(selected_embeddings, sample_embedding)\n",
    "    distance_vector = 1.0 - similarity_vector\n",
    "\n",
    "    if selected_embeddings.size(0) > k_near:\n",
    "        distance_vector, _ = torch.topk(distance_vector, k=k_near, dim=0)\n",
    "    \n",
    "    mean_distance = distance_vector.mean().item()  # .item()确保从张量转换为python标量\n",
    "    return mean_distance\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def process_dialog(dialog):\n",
    "    conversation = \"\"\n",
    "    for message in dialog['messages']:\n",
    "        conversation += f\"### {message['role']}: {message['content']}\\n\"\n",
    "    return {\"features\": conversation}\n",
    "\n",
    "def embed_text(batch):\n",
    "    encoded_inputs = tokenizer(batch['features'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**encoded_inputs)\n",
    "    sentence_embeddings = mean_pooling(model_outputs, encoded_inputs['attention_mask'])\n",
    "    embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    batch['embeddings'] = embeddings.cpu().numpy().tolist()  # 确保从GPU传递回CPU进行保存\n",
    "    return batch\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dataset_name = 'all_train'\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size = 40000\n",
    "\n",
    "threshold = 0.5\n",
    "k_near = 10\n",
    "\n",
    "if not os.path.exists(f\"{dataset_name}_embeddings.parquet\"):\n",
    "    data = load_dataset('json', data_files=f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/data/train_data/{dataset_name}_data.jsonl\")\n",
    "    data['train'] = data['train'].map(process_dialog, batched=False)\n",
    "\n",
    "    embedding_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    data['train'] = data['train'].map(embed_text, batched=True, batch_size=2048)\n",
    "    data['train'].to_parquet(f'{dataset_name}_embeddings.parquet')\n",
    "    print(f\"Embeddings saved to {dataset_name}_embeddings.parquet\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "# 定义标签和优先级\n",
    "label_priority = [5, 4, 3, 2]\n",
    "label_indices = {label: [idx for idx, lbl in enumerate(labels) if lbl == label] for label in label_priority}\n",
    "\n",
    "embedding_dataset = load_dataset('parquet', data_files=f'{dataset_name}_embeddings.parquet')['train']\n",
    "\n",
    "# 选择高质量样本\n",
    "selected_embeddings = None\n",
    "selected_samples = []\n",
    "\n",
    "for label in label_priority:\n",
    "    if len(selected_samples) >= dataset_size:\n",
    "        break\n",
    "    \n",
    "    embedding_subset = embedding_dataset.select(label_indices[label])\n",
    "    \n",
    "    for sample in tqdm(embedding_subset, desc=f\"Processing high-quality samples (label={label})\"):\n",
    "        sample_embedding = torch.tensor(sample['embeddings']).to(device)  # 确保 sample_embedding 在 GPU 上\n",
    "        \n",
    "        if selected_embeddings is None:\n",
    "            selected_embeddings = sample_embedding.unsqueeze(0)\n",
    "            selected_samples.append(sample)\n",
    "            continue\n",
    "        \n",
    "        if cosDistance(sample_embedding, selected_embeddings, k_near=k_near) < threshold:\n",
    "            selected_embeddings = torch.cat((selected_embeddings, sample_embedding.unsqueeze(0)), dim=0)\n",
    "            selected_samples.append(sample)\n",
    "        \n",
    "        if len(selected_samples) >= dataset_size:\n",
    "            break\n",
    "    print(f\"Selected data size from label {label}: {len(selected_samples)}\")\n",
    "\n",
    "selected_dataset = Dataset.from_dict({col: [s[col] for s in selected_samples] for col in selected_samples[0].keys()})\n",
    "print(f\"Selected data size: {len(selected_dataset)}\")\n",
    "\n",
    "selected_dataset.to_json(root_path + f'diversity-filtered-{dataset_size//1000}k_dataset.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filetered method: Our Data selection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the label curation info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered 5: label-filtered based: all 5 samples  + 4-rated samples select using sorted long-tail score (reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cured sample size: 0\n",
      "Corrupted samples total: 119476\n",
      "Original Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "Label-wise filter out samples: 119476\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Finished caching labels indices...\n",
      "Size of the filtered dataset: 487\n",
      "Size of the filtered dataset: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 29.15ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101209769"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name ='all_train'\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size = 40000\n",
    "\n",
    "# label curation reports\n",
    "report_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/labeling/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= 1:  # confidence prob\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"/home/azureuser/cloudfiles/code/Users/jinlong.pang/LADR_LLM_alignment_data_refinement/open-instruct/model_finetune_cluster/new_train_data/{model_name}/{dataset_name}/\"\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "# Part 2 (feature-wise): handle the rare example\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "# rare_samples_filtered = [[sample[0], sample[1]] for sample in rare_samples]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # 使用NumPy进行操作\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 缓存标签索引\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(f\"Finished caching labels indices...\")\n",
    "\n",
    "filtered_indices = []\n",
    "\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # 获取对应标签的样本并排序\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # 确保样本不为空\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "    \n",
    "\n",
    "# Filter out the corrupted samples and reconstruct the dataset\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "# 根据 filtered_indices 选择样本\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "\n",
    "# 保存过滤后的数据集\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-{dataset_size//1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     3,     30,     56, ..., 300921, 300927, 300929])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Part 2 (feature-wise): 处理稀有样本\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # 使用NumPy进行操作\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "# 假设 labels 是 Python 列表，将其转换为 NumPy 数组\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 缓存标签索引\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(f\"Finished caching labels indices...\")\n",
    "\n",
    "filtered_indices = []\n",
    "\n",
    "# 过滤样本并进行排序\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # 获取对应标签的样本并排序\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # 确保样本不为空\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# 加载数据集并过滤掉无效样本\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "# 选择样本并保存\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-cured-{dataset_size//1000}k_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
