{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label curation \n",
    "\n",
    "- First Step to initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/Docta/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1538163/3981394206.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reports = torch.load(report_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "Cured sample size: 10510\n",
      "Corrupted samples total: 112457\n",
      "Original Counter(labels): Counter({3: 87975, 2: 86132, 4: 59969, 1: 44401, 0: 18626, 5: 3829})\n",
      "counting revised label size: 10510\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 89665, 2: 86085, 4: 61057, 1: 43117, 0: 18503, 5: 2505})\n",
      "Label-wise filter out samples: 112457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538163/3981394206.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(root_path + \"output_labels_revised.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "dataset_size = 10000\n",
    "confidence_prob = 0.5\n",
    "\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"full_dataset.json\")\n",
    "\n",
    "#################################################################################################################################\n",
    "# label curation reports\n",
    "report_path = f\"score_curation/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= confidence_prob:  # confidence prob\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((int(sample[0]), int(sample[1]), round(sample[2],2)))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"../model_finetune/selected_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "count=0\n",
    "#identify the transition labels\n",
    "count_labels_5 = []\n",
    "count_labels_4 = []\n",
    "count_labels_3 = []\n",
    "count_labels_2 = []\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    if labels[sample_label[0]] == 5:\n",
    "        count_labels_5.append(sample_label)\n",
    "        # continue ## determine whether remain the 5-rated samples\n",
    "\n",
    "    if labels[sample_label[0]] == 4:\n",
    "        count_labels_4.append(sample_label)\n",
    "\n",
    "    if labels[sample_label[0]] == 3:\n",
    "        count_labels_3.append(sample_label)\n",
    "\n",
    "    if labels[sample_label[0]] == 2:\n",
    "        count_labels_2.append(sample_label)\n",
    "\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "    count+=1\n",
    "\n",
    "print(f\"counting revised label size: {count}\")\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Revised Counter(labels): {label_counts}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "### load the label noise \n",
    "# torch.save(labels, root_path + f\"output_labels_revised_cured.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Label-filtered algorithm \n",
    "\n",
    "- tag: label-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 38.40ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16755007"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "label_filtered_indices = []\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(label_filtered_indices) == dataset_size:\n",
    "        break   \n",
    "\n",
    "    indices = [i for i, label in enumerate(labels) if label == target_label]\n",
    "\n",
    "    if dataset_size - len(label_filtered_indices) > len(indices):\n",
    "        label_filtered_indices.extend(indices)\n",
    "    else:\n",
    "        random_indices = np.random.permutation(len(indices))[:dataset_size-len(label_filtered_indices)]\n",
    "        label_filtered_indices.extend(random_indices)\n",
    "\n",
    "print(f\"data size: {len(label_filtered_indices)}\")\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"./model_finetune/selected_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"label-filtered-cured-{confidence_prob}_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: diversity-filtered\n",
    "\n",
    "- tag: diversity-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=5): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 487/487 [00:06<00:00, 73.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 5: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66181/66181 [01:47<00:00, 617.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 4: 8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=3):   5%|██████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 5460/118313 [00:04<01:40, 1125.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 3: 10000\n",
      "Selected data size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.72ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177782124"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def cosDistance(sample_embedding, selected_embeddings, k_near=10):\n",
    "    sample_embedding = sample_embedding.to(device)\n",
    "    selected_embeddings = selected_embeddings.to(device)\n",
    "\n",
    "    similarity_vector = torch.matmul(selected_embeddings, sample_embedding)\n",
    "    distance_vector = 1.0 - similarity_vector\n",
    "\n",
    "    if selected_embeddings.size(0) > k_near:\n",
    "        distance_vector, _ = torch.topk(distance_vector, k=k_near, dim=0)\n",
    "    \n",
    "    mean_distance = distance_vector.mean().item()  #\n",
    "    return mean_distance\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def process_dialog(dialog):\n",
    "    conversation = \"\"\n",
    "    for message in dialog['messages']:\n",
    "        conversation += f\"### {message['role']}: {message['content']}\\n\"\n",
    "    return {\"features\": conversation}\n",
    "\n",
    "def embed_text(batch):\n",
    "    encoded_inputs = tokenizer(batch['features'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**encoded_inputs)\n",
    "    sentence_embeddings = mean_pooling(model_outputs, encoded_inputs['attention_mask'])\n",
    "    embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    batch['embeddings'] = embeddings.cpu().numpy().tolist()  \n",
    "    return batch\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "threshold = 0.5\n",
    "k_near = 10\n",
    "\n",
    "if not os.path.exists(f\"{dataset_name}_embeddings.parquet\"):\n",
    "    data = load_dataset('json', data_files=f\"./data/train_data/{dataset_name}_data.jsonl\")\n",
    "    data['train'] = data['train'].map(process_dialog, batched=False)\n",
    "\n",
    "    embedding_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    data['train'] = data['train'].map(embed_text, batched=True, batch_size=2048)\n",
    "    data['train'].to_parquet(f'{dataset_name}_embeddings.parquet')\n",
    "    print(f\"Embeddings saved to {dataset_name}_embeddings.parquet\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "label_priority = [5, 4, 3, 2, 1]\n",
    "label_indices = {label: [idx for idx, lbl in enumerate(labels) if lbl == label] for label in label_priority}\n",
    "\n",
    "embedding_dataset = load_dataset('parquet', data_files=f'{dataset_name}_embeddings.parquet')['train']\n",
    "\n",
    "selected_embeddings = None\n",
    "selected_samples = []\n",
    "\n",
    "for label in label_priority:\n",
    "    if len(selected_samples) >= dataset_size:\n",
    "        break\n",
    "    \n",
    "    embedding_subset = embedding_dataset.select(label_indices[label])\n",
    "    \n",
    "    for sample in tqdm(embedding_subset, desc=f\"Processing high-quality samples (label={label})\"):\n",
    "        sample_embedding = torch.tensor(sample['embeddings']).to(device)  \n",
    "        \n",
    "        if selected_embeddings is None:\n",
    "            selected_embeddings = sample_embedding.unsqueeze(0)\n",
    "            selected_samples.append(sample)\n",
    "            continue\n",
    "        \n",
    "        if cosDistance(sample_embedding, selected_embeddings, k_near=k_near) < threshold:\n",
    "            selected_embeddings = torch.cat((selected_embeddings, sample_embedding.unsqueeze(0)), dim=0)\n",
    "            selected_samples.append(sample)\n",
    "        \n",
    "        if len(selected_samples) >= dataset_size:\n",
    "            break\n",
    "    print(f\"Selected data size from label {label}: {len(selected_samples)}\")\n",
    "\n",
    "selected_dataset = Dataset.from_dict({col: [s[col] for s in selected_samples] for col in selected_samples[0].keys()})\n",
    "print(f\"Selected data size: {len(selected_dataset)}\")\n",
    "\n",
    "selected_dataset.to_json(root_path + f'diversity-filtered-cured-{confidence_prob}_dataset.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filetered method: Our Data selection method\n",
    "- Filtered 5: label-filtered based: all 5 samples  + 4-rated samples select using sorted long-tail score (reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Part 2 (feature-wise): Process rare samples based on 'rare_example' detection\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # Use NumPy for faster operations\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "# Assume 'labels' is a Python list; convert it to a NumPy array for efficient indexing\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cache label indices to avoid repeated searches\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(f\"Finished caching labels indices...\")\n",
    "\n",
    "# Initialize list to store selected indices\n",
    "filtered_indices = []\n",
    "\n",
    "# Filter and sort samples by label\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    # Get indices of current label\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    # Add label indices if enough space, else sort and add top samples\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # Filter and sort samples with the target label by score\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # Ensure label_samples is not empty\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# Load the dataset and filter out samples by selected indices\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "# Select and save filtered samples\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-cured-{confidence_prob}_dataset.json\")\n",
    "print(f\"Filtered dataset saved to {root_path}filtered-cured-{confidence_prob}_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
