{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Baseline only for test: Combined\n",
    "\n",
    "- Baseline: Combine\n",
    "- We contatenate all high-rated samples from three labeling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "seed = 3\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "\n",
    "#llama labels\n",
    "model_full_name ='meta-llama/Meta-Llama-3.1-8B-Instruct/'\n",
    "new_train_dir = f\"./model_finetune/new_train_data/{model_full_name}\"\n",
    "output_file = os.path.join(new_train_dir, 'all_train/output_labels_revised.pt')\n",
    "llama_labels= torch.load(output_file)\n",
    "\n",
    "\n",
    "llama_labels_dict = defaultdict(list)\n",
    "for index, label in enumerate(llama_labels):\n",
    "    llama_labels_dict[label].append(index)\n",
    "\n",
    "\n",
    "##GPT 4o-mini labels\n",
    "model_full_name='gpt-4o-mini'\n",
    "new_train_dir = f\"./model_finetune/new_train_data/{model_full_name}\"\n",
    "output_file = os.path.join(new_train_dir, 'all_train/output_labels_revised.pt')\n",
    "gpt_labels= torch.load(output_file)\n",
    "\n",
    "gpt_labels_dict = defaultdict(list)\n",
    "for index, label in enumerate(gpt_labels):\n",
    "    gpt_labels_dict[label].append(index)\n",
    "\n",
    "\n",
    "#mistral labels\n",
    "model_full_name='mistralai/Mistral-7B-Instruct-v0.3'\n",
    "new_train_dir = f\"./model_finetune/new_train_data/{model_full_name}\"\n",
    "output_file = os.path.join(new_train_dir, 'all_train/output_labels_revised.pt')\n",
    "mistral_labels= torch.load(output_file)\n",
    "\n",
    "\n",
    "mistral_labels_dict = defaultdict(list)\n",
    "for index, label in enumerate(mistral_labels):\n",
    "    mistral_labels_dict[label].append(index)\n",
    "\n",
    "\n",
    "\n",
    "intersection_all_dict = {}\n",
    "\n",
    "for label in [5,4,3,2,1]:\n",
    "    intersection_gpt_llama = set(gpt_labels_dict[label]) & set(llama_labels_dict[label])\n",
    "    intersection_gpt_mistral = set(gpt_labels_dict[label]) & set(mistral_labels_dict[label])\n",
    "    intersection_llama_mistral = set(llama_labels_dict[label]) & set(mistral_labels_dict[label])\n",
    "    intersection_all_dict[label] = intersection_gpt_llama & intersection_gpt_mistral & intersection_llama_mistral\n",
    "    print(f\"intersection_all: {len(intersection_all_dict[label])} --- intersection_gpt_llama size: {len(intersection_gpt_llama)} -- intersection_gpt_mistral size: {len(intersection_gpt_mistral)} -- intersection_llama_mistral size:{len(intersection_llama_mistral)}\")\n",
    "\n",
    "\n",
    "dataset_size = 10000\n",
    "combined_label_indices = set(llama_labels_dict[5] + mistral_labels_dict[5] + gpt_labels_dict[5])\n",
    "\n",
    "additional_label_indices = random.sample(intersection_all_dict[4], dataset_size-len(combined_label_indices))\n",
    "\n",
    "\n",
    "ideal_label_indices = list(combined_label_indices) + additional_label_indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#### \n",
    "dataset_size = 10000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "json_dir = './data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(ideal_label_indices)\n",
    "\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "random_dataset.to_json(root_path + f\"combined_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
