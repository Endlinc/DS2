{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered method\n",
    "\n",
    "- Dataset: stanford_alpaca\n",
    "- Ours filtered curation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 52002 examples [00:00, 291127.96 examples/s]\n",
      "/tmp/ipykernel_2145892/1611418300.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reports = torch.load(report_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "Cured sample size: 16513\n",
      "Corrupted samples total: 1004\n",
      "Original Counter(labels): Counter({2: 18624, 1: 15973, 0: 8274, 3: 8017, 4: 1092, 5: 22})\n",
      "counting revised label size: 16513\n",
      "Label size: 52002\n",
      "Revised Counter(labels): Counter({2: 19470, 1: 18728, 0: 8635, 3: 4741, 4: 422, 5: 6})\n",
      "Label-wise filter out samples: 1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2145892/1611418300.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(root_path + \"output_labels_revised.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name='stanford_alpaca'\n",
    "# model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "dataset_size = 9000\n",
    "confidence_prob = 0.3\n",
    "\n",
    "root_path = f\"../model_finetune/selected_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =root_path + \"full_dataset.json\")\n",
    "\n",
    "#################################################################################################################################\n",
    "# label curation reports\n",
    "# report_path = f\"score_curation/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "\n",
    "report_path = f\"results-embedding-model-bge/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "\n",
    "\n",
    "\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1 (label-wise): label curation\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= confidence_prob:  # confidence prob\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((int(sample[0]), int(sample[1]), round(sample[2],2)))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "\n",
    "\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "count=0\n",
    "#identify the transition labels\n",
    "count_labels_5 = []\n",
    "count_labels_4 = []\n",
    "count_labels_3 = []\n",
    "count_labels_2 = []\n",
    "\n",
    "for sample_label in cured_sample_labels:\n",
    "    if labels[sample_label[0]] == 5:\n",
    "        count_labels_5.append(sample_label)\n",
    "        # continue ## determine whether remain the 5-rated samples\n",
    "\n",
    "    if labels[sample_label[0]] == 4:\n",
    "        count_labels_4.append(sample_label)\n",
    "\n",
    "    if labels[sample_label[0]] == 3:\n",
    "        count_labels_3.append(sample_label)\n",
    "\n",
    "    if labels[sample_label[0]] == 2:\n",
    "        count_labels_2.append(sample_label)\n",
    "\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "    count+=1\n",
    "\n",
    "print(f\"counting revised label size: {count}\")\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Revised Counter(labels): {label_counts}\")\n",
    "\n",
    "# Filter out the low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "### load the label noise \n",
    "# torch.save(labels, root_path + f\"output_labels_revised_cured.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Part 2 (feature-wise): Process rare samples based on 'rare_example' detection\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # Use NumPy for faster operations\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "# Assume 'labels' is a Python list; convert it to a NumPy array for efficient indexing\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cache label indices to avoid repeated searches\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(f\"Finished caching labels indices...\")\n",
    "\n",
    "# Initialize list to store selected indices\n",
    "filtered_indices = []\n",
    "\n",
    "# Filter and sort samples by label\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    # Get indices of current label\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    # Add label indices if enough space, else sort and add top samples\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # Filter and sort samples with the target label by score\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # Ensure label_samples is not empty\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# Load the dataset and filter out samples by selected indices\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "# Select and save filtered samples\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-cured-{confidence_prob}_dataset.json\")\n",
    "print(f\"Filtered dataset saved to {root_path}filtered-cured-{confidence_prob}_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
