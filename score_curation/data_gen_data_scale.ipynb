{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 301/301 [00:04<00:00, 73.91ba/s]\n",
      "Creating json from Arrow format: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 46.72ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4339289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "######\n",
    "dataset_size = 5000\n",
    "dataset_name ='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "json_dir = './data/train_data/'\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files=json_dir+'all_train_data.jsonl')['train']\n",
    "\n",
    "random_indices = np.random.permutation(len(all_train_dataset))[:dataset_size]\n",
    "\n",
    "random_dataset = all_train_dataset.select(random_indices)\n",
    "\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "all_train_dataset.to_json(root_path + f\"full_dataset.json\")\n",
    "\n",
    "random_dataset.to_json(root_path + f\"random-{dataset_size//1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: LESS (Influence-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "task='mmlu'\n",
    "tot_dataset_name='all_train'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "dataset_size =40000\n",
    "\n",
    "\n",
    "DATASET_LIST = ['flan_v2', 'oasst1', 'wizardlm', 'dolly', 'stanford_alpaca']\n",
    "task_list = [\"mmlu\", \"bbh\", \"truthfulqa\", \"gsm\", \"tydiqa\"]\n",
    "\n",
    "infl_score_all_tasks = []\n",
    "for task in task_list:\n",
    "    infl_score_all = []\n",
    "    for dataset_name in DATASET_LIST:\n",
    "        root_path = f\"less-outputs/selected_data/{task}/\"\n",
    "        infl_scores = torch.load(root_path + f\"{dataset_name}_influence_score.pt\")        \n",
    "        infl_score_all.append(infl_scores)\n",
    "\n",
    "    infl_score_all = torch.cat(infl_score_all, dim=0)\n",
    "    infl_score_all_tasks.append(infl_score_all)\n",
    "    \n",
    "infl_score_all_tasks = torch.stack(infl_score_all_tasks).mean(dim=0)\n",
    "print(f\"final infl score's shape: {infl_score_all_tasks.shape}\")\n",
    "\n",
    "values, indices = torch.topk(infl_score_all_tasks, k=dataset_size)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{tot_dataset_name}/\"\n",
    "\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "less_dataset = data['train'].select(indices)\n",
    "\n",
    "less_dataset.to_json(root_path + f\"less-{dataset_size//1000}k_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Label-filtered algorithm \n",
    "\n",
    "- tag: label-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "data size: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 42.76ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66928993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "seed =3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name='all_train'\n",
    "# model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name=\"gpt-4o-mini\"\n",
    "model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "dataset_size = 40000\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"./data/train_data/{dataset_name}_data.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "label_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/output_labels_revised.pt\"\n",
    "labels = torch.load(label_path)\n",
    "\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f\"Counter(labels): {label_counts}\")\n",
    "\n",
    "\n",
    "label_filtered_indices = []\n",
    "for target_label in label_counts.keys():\n",
    "    if len(label_filtered_indices) == dataset_size:\n",
    "        break   \n",
    "\n",
    "    indices = [i for i, label in enumerate(labels) if label == target_label]\n",
    "\n",
    "    if dataset_size - len(label_filtered_indices) > len(indices):\n",
    "        label_filtered_indices.extend(indices)\n",
    "    else:\n",
    "        random_indices = np.random.permutation(len(indices))[:dataset_size-len(label_filtered_indices)]\n",
    "        label_filtered_indices.extend(random_indices)\n",
    "\n",
    "print(f\"data size: {len(label_filtered_indices)}\")\n",
    "\n",
    "label_filtered_dataset = all_train_dataset['train'].select(label_filtered_indices)\n",
    "\n",
    "label_filtered_labels = np.array(labels)[label_filtered_indices].tolist()\n",
    "\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/\"\n",
    "\n",
    "label_filtered_dataset.to_json(root_path + f\"label-filtered-{dataset_size//1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: diversity-filtered\n",
    "\n",
    "- tag: diversity-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=5): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3829/3829 [00:03<00:00, 1216.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 5: 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59969/59969 [00:48<00:00, 1223.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 4: 6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=3): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87975/87975 [01:22<00:00, 1060.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 3: 25099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high-quality samples (label=2):  52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 44717/86132 [00:46<00:42, 970.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data size from label 2: 40000\n",
      "Selected data size: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:07<00:00,  5.70ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "713261521"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def cosDistance(sample_embedding, selected_embeddings, k_near=10):\n",
    "    sample_embedding = sample_embedding.to(device)\n",
    "    selected_embeddings = selected_embeddings.to(device)\n",
    "\n",
    "    similarity_vector = torch.matmul(selected_embeddings, sample_embedding)\n",
    "    distance_vector = 1.0 - similarity_vector\n",
    "\n",
    "    if selected_embeddings.size(0) > k_near:\n",
    "        distance_vector, _ = torch.topk(distance_vector, k=k_near, dim=0)\n",
    "    \n",
    "    mean_distance = distance_vector.mean().item()  \n",
    "    return mean_distance\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def process_dialog(dialog):\n",
    "    conversation = \"\"\n",
    "    for message in dialog['messages']:\n",
    "        conversation += f\"### {message['role']}: {message['content']}\\n\"\n",
    "    return {\"features\": conversation}\n",
    "\n",
    "def embed_text(batch):\n",
    "    encoded_inputs = tokenizer(batch['features'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**encoded_inputs)\n",
    "    sentence_embeddings = mean_pooling(model_outputs, encoded_inputs['attention_mask'])\n",
    "    embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    batch['embeddings'] = embeddings.cpu().numpy().tolist() \n",
    "    return batch\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dataset_name = 'all_train'\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "\n",
    "dataset_size = 40000\n",
    "\n",
    "threshold = 0.5\n",
    "k_near = 10\n",
    "\n",
    "if not os.path.exists(f\"{dataset_name}_embeddings.parquet\"):\n",
    "    data = load_dataset('json', data_files=f\"./data/train_data/{dataset_name}_data.jsonl\")\n",
    "    data['train'] = data['train'].map(process_dialog, batched=False)\n",
    "\n",
    "    embedding_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    data['train'] = data['train'].map(embed_text, batched=True, batch_size=2048)\n",
    "    data['train'].to_parquet(f'{dataset_name}_embeddings.parquet')\n",
    "    print(f\"Embeddings saved to {dataset_name}_embeddings.parquet\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/\"\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "\n",
    "label_priority = [5, 4, 3, 2]\n",
    "label_indices = {label: [idx for idx, lbl in enumerate(labels) if lbl == label] for label in label_priority}\n",
    "\n",
    "embedding_dataset = load_dataset('parquet', data_files=f'{dataset_name}_embeddings.parquet')['train']\n",
    "\n",
    "\n",
    "selected_embeddings = None\n",
    "selected_samples = []\n",
    "\n",
    "for label in label_priority:\n",
    "    if len(selected_samples) >= dataset_size:\n",
    "        break\n",
    "    \n",
    "    embedding_subset = embedding_dataset.select(label_indices[label])\n",
    "    \n",
    "    for sample in tqdm(embedding_subset, desc=f\"Processing high-quality samples (label={label})\"):\n",
    "        sample_embedding = torch.tensor(sample['embeddings']).to(device) \n",
    "        \n",
    "        if selected_embeddings is None:\n",
    "            selected_embeddings = sample_embedding.unsqueeze(0)\n",
    "            selected_samples.append(sample)\n",
    "            continue\n",
    "        \n",
    "        if cosDistance(sample_embedding, selected_embeddings, k_near=k_near) < threshold:\n",
    "            selected_embeddings = torch.cat((selected_embeddings, sample_embedding.unsqueeze(0)), dim=0)\n",
    "            selected_samples.append(sample)\n",
    "        \n",
    "        if len(selected_samples) >= dataset_size:\n",
    "            break\n",
    "    print(f\"Selected data size from label {label}: {len(selected_samples)}\")\n",
    "\n",
    "selected_dataset = Dataset.from_dict({col: [s[col] for s in selected_samples] for col in selected_samples[0].keys()})\n",
    "print(f\"Selected data size: {len(selected_dataset)}\")\n",
    "\n",
    "selected_dataset.to_json(root_path + f'diversity-filtered-{dataset_size//1000}k_dataset.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filetered method: Our Data selection method\n",
    "\n",
    "- Filtered 5: label-filtered based: all 5 samples  + 4-rated samples select using sorted long-tail score (reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cured sample size: 0\n",
      "Corrupted samples total: 119476\n",
      "Original Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 118313, 4: 66181, 2: 60607, 1: 52908, 0: 2436, 5: 487})\n",
      "Label-wise filter out samples: 119476\n",
      "Size of the remaining samples with high quality: 300932\n",
      "Finished caching labels indices...\n",
      "Size of the filtered dataset: 487\n",
      "Size of the filtered dataset: 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 29.15ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101209769"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Dataset and model information\n",
    "dataset_name = 'all_train'\n",
    "model_name = \"gpt-4o-mini\"\n",
    "dataset_size = 40000\n",
    "\n",
    "# Load label curation report\n",
    "report_path = f\"score_curation/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "# Part 1: Label curation based on detection of label errors\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "cured_samples = []\n",
    "cured_sample_labels = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= 1:  # Filter by confidence threshold\n",
    "        cured_samples.append(sample[0])\n",
    "        cured_sample_labels.append((sample[0], sample[1]))\n",
    "\n",
    "print(f\"Cured sample size: {len(cured_sample_labels)}\")\n",
    "\n",
    "# Exclude cured samples from corrupted samples list\n",
    "cured_samples_set = set(cured_samples)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in cured_samples_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change original labels to cured labels\n",
    "root_path = f\"./model_finetune/new_train_data/{model_name}/{dataset_name}/\"\n",
    "labels = torch.load(root_path + \"output_labels_revised.pt\")\n",
    "print(f\"Original Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Apply cured labels to original labels\n",
    "for sample_label in cured_sample_labels:\n",
    "    labels[sample_label[0]] = sample_label[1]\n",
    "\n",
    "print(f\"Label size: {len(labels)}\")\n",
    "print(f\"Revised Counter(labels): {Counter(labels)}\")\n",
    "\n",
    "# Filter out low-quality samples\n",
    "label_wise_filter_out_samples = set(corrupted_samples_total)\n",
    "print(f\"Label-wise filter out samples: {len(label_wise_filter_out_samples)}\")\n",
    "\n",
    "# Part 2: Handle rare examples based on feature-wise detection\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # Convert to NumPy array for fast operations\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "# Convert labels to a NumPy array for efficient indexing\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cache indices for each label to avoid repeated searches\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(\"Finished caching labels indices.\")\n",
    "\n",
    "# Initialize list for storing selected indices\n",
    "filtered_indices = []\n",
    "\n",
    "# Filter and sort samples by label, starting with the highest quality\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    # Retrieve indices of the current label from cache\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    # Add all samples if there's space, otherwise select top samples\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # Filter rare samples for the current label\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # Ensure label_samples is non-empty\n",
    "            # Sort by score (descending) and add top samples\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# Load the dataset and select samples based on filtered indices\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "\n",
    "# Save the filtered dataset as JSON\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-{dataset_size // 1000}k_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered curated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Part 2 (feature-wise): Process rare samples\n",
    "# Get the top 50% rare samples based on the list in 'reports'\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # Use NumPy for efficient operations\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "# Assume 'labels' is a list; convert it to a NumPy array for faster indexing\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cache label indices to avoid repeated searching\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "print(f\"Finished caching labels indices...\")\n",
    "\n",
    "# Initialize an empty list to store selected indices\n",
    "filtered_indices = []\n",
    "\n",
    "# Filter and sort samples by label, prioritizing high-quality samples\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    # Get indices of the current label from cache\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    # Add all label indices if there is space; otherwise, add only top samples\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # Filter rare samples for the current label\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  # Ensure samples are non-empty\n",
    "            # Sort samples by score (descending) and select top samples\n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "# Load the dataset and filter based on selected indices\n",
    "data = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "\n",
    "# Select samples by filtered indices and save to JSON\n",
    "filtered_dialogs = data['train'].select(filtered_indices)\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-cured-{dataset_size // 1000}k_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
