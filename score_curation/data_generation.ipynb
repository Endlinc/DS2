{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score curation \n",
    "\n",
    "- Use the score transition matrix (report) to implement score curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/Docta/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1538163/3981394206.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reports = torch.load(report_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Docta: Doctor for your data. Current version: 0.2 ====\n",
      "Cured sample size: 10510\n",
      "Corrupted samples total: 112457\n",
      "Original Counter(labels): Counter({3: 87975, 2: 86132, 4: 59969, 1: 44401, 0: 18626, 5: 3829})\n",
      "counting revised label size: 10510\n",
      "Label size: 300932\n",
      "Revised Counter(labels): Counter({3: 89665, 2: 86085, 4: 61057, 1: 43117, 0: 18503, 5: 2505})\n",
      "Label-wise filter out samples: 112457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538163/3981394206.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(root_path + \"output_labels_revised.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from collections import Counter\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "seed=3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_name='dolly'\n",
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "dataset_size = 10000\n",
    "confidence_prob = 0.5\n",
    "\n",
    "all_train_dataset = load_dataset('json', data_files =f\"{dataset_name}.json\") #300k data\n",
    "\n",
    "\n",
    "# label curation reports\n",
    "report_path = f\"score_curation/results/{model_name}/{dataset_name}/{dataset_name}_report.pt\"\n",
    "reports = torch.load(report_path)\n",
    "\n",
    "corrupted_samples = [x[0] for x in reports.detection['label_error']]\n",
    "\n",
    "curated_sample = []\n",
    "curated_sample_scores = []\n",
    "for sample in reports.curation['label_curation']:  # (idx, label, confidence)\n",
    "    if sample[2] >= confidence_prob:  \n",
    "        curated_sample.append(sample[0])\n",
    "        curated_sample_scores.append((int(sample[0]), int(sample[1]), round(sample[2],2)))\n",
    "\n",
    "print(f\"Curated sample size: {len(curated_sample_scores)}\")\n",
    "\n",
    "# Filter out some cured samples from corrupted instances\n",
    "curated_sample_set = set(curated_sample)\n",
    "corrupted_samples_total = [x for x in corrupted_samples if x not in curated_sample_set]\n",
    "\n",
    "print(f\"Corrupted samples total: {len(corrupted_samples_total)}\")\n",
    "\n",
    "# Change the original labels to the suggested label\n",
    "root_path = f\"../model_finetune/selected_data/{model_name}/{dataset_name}/\"\n",
    "scores = torch.load(root_path + \"output_scores.pt\")\n",
    "\n",
    "for sample_score in curated_sample_scores:\n",
    "    scores[sample_score[0]] = sample_score[1]\n",
    "\n",
    "\n",
    "### load the score noise \n",
    "torch.save(scores, root_path + f\"output_scores_curated.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Part 2 (feature-wise): Process rare samples based on 'rare_example' detection\n",
    "rare_samples = reports.detection['rare_example'][:len(reports.detection['rare_example']) // 2]\n",
    "rare_samples_filtered = np.array(rare_samples)[:, :2]  # Use NumPy for faster operations\n",
    "\n",
    "print(f\"Size of the remaining samples with high quality: {len(rare_samples_filtered)}\")\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cache label indices to avoid repeated searches\n",
    "label_indices_cache = {label: np.where(labels == label)[0] for label in [5, 4, 3, 2, 1]}\n",
    "\n",
    "# Initialize list to store selected indices\n",
    "filtered_indices = []\n",
    "\n",
    "# Filter and sort samples by label\n",
    "for target_label in [5, 4, 3, 2, 1]:\n",
    "    if len(filtered_indices) >= dataset_size:\n",
    "        break\n",
    "\n",
    "    # Get indices of current label\n",
    "    label_indices = label_indices_cache[target_label]\n",
    "    available_size = dataset_size - len(filtered_indices)\n",
    "\n",
    "    # Add label indices if enough space, else sort and add top samples\n",
    "    if available_size > len(label_indices):\n",
    "        filtered_indices.extend(label_indices.tolist())\n",
    "    else:\n",
    "        # Filter and sort samples with the target label by score\n",
    "        label_samples = rare_samples_filtered[np.isin(rare_samples_filtered[:, 0], label_indices)]\n",
    "        if len(label_samples) > 0:  \n",
    "            sorted_samples = label_samples[label_samples[:, 1].argsort()[::-1]][:available_size]\n",
    "            filtered_indices.extend(sorted_samples[:, 0].astype(int).tolist())\n",
    "\n",
    "    print(\"Size of the filtered dataset:\", len(filtered_indices))\n",
    "\n",
    "\n",
    "# Load the dataset and filter out samples by selected indices\n",
    "raw_dataset = load_dataset('json', data_files=root_path + 'full_dataset.json')\n",
    "filtered_dialogs = raw_dataset['train'].select(filtered_indices)\n",
    "filtered_dialogs.to_json(root_path + f\"filtered-curated_dataset.json\")\n",
    "print(f\"Final dataset saved to {root_path}filtered-curated_dataset.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
